# ğŸ–¥ï¸ å¤š GPU ä»¿çœŸ

Genesis æ”¯æŒå¤š GPU æ‰§è¡Œä»¥æ‰©å±•ä»¿çœŸã€‚

## å• GPU é…ç½®

```python
import genesis as gs

# è‡ªåŠ¨ GPU é€‰æ‹©
gs.init(backend=gs.gpu)

# å¼ºåˆ¶ç‰¹å®šåç«¯
gs.init(backend=gs.cuda)   # NVIDIA CUDA
gs.init(backend=gs.metal)  # Apple Metal
gs.init(backend=gs.cpu)    # CPU å›é€€
```

## å¹¶è¡Œç¯å¢ƒï¼ˆå• GPUï¼‰

é€šè¿‡åœ¨å•ä¸ª GPU ä¸Šæ‰¹å¤„ç†ç¯å¢ƒæ¥æ‰©å±•ï¼š

```python
scene.build(n_envs=2048, env_spacing=(1.0, 1.0))
# æ‰€æœ‰ç¯å¢ƒåœ¨åŒä¸€ GPU ä¸Šå¹¶è¡Œè¿è¡Œ
```

## ä½¿ç”¨å¤šè¿›ç¨‹çš„å¤š GPU

æ¯ GPU è¿è¡Œå•ç‹¬çš„è¿›ç¨‹ï¼š

```python
import os
import multiprocessing

def run_simulation(gpu_id):
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    os.environ["TI_VISIBLE_DEVICE"] = str(gpu_id)
    os.environ["EGL_DEVICE_ID"] = str(gpu_id)

    import genesis as gs
    gs.init(backend=gs.gpu)
    # ... ä»¿çœŸä»£ç  ...

if __name__ == "__main__":
    for i in range(2):  # 2 ä¸ª GPU
        p = multiprocessing.Process(target=run_simulation, args=(i,))
        p.start()
```

## åˆ†å¸ƒå¼è®­ç»ƒ (DDP)

ä½¿ç”¨ PyTorch åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼š

```bash
torchrun --standalone --nnodes=1 --nproc_per_node=2 train.py
```

```python
import os
import torch
import torch.distributed as dist
import genesis as gs

local_rank = int(os.environ.get("LOCAL_RANK", 0))
os.environ["CUDA_VISIBLE_DEVICES"] = str(local_rank)
os.environ["TI_VISIBLE_DEVICE"] = str(local_rank)

gs.init(backend=gs.gpu, seed=local_rank)
scene.build(n_envs=2048)

torch.cuda.set_device(0)
dist.init_process_group(backend="nccl", init_method="env://")
model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[0])

# å¸¦æ¢¯åº¦åŒæ­¥çš„è®­ç»ƒå¾ªç¯
for step in range(steps):
    scene.step()
    loss.backward()  # DDP å¤„ç† all-reduce
    optimizer.step()

dist.barrier()
dist.destroy_process_group()
```

## ç¯å¢ƒå˜é‡

| å˜é‡ | ç›®çš„ |
|----------|---------|
| `CUDA_VISIBLE_DEVICES` | PyTorch/CUDA GPU é€‰æ‹© |
| `TI_VISIBLE_DEVICE` | Taichi GPU é€‰æ‹© |
| `EGL_DEVICE_ID` | æ¸²æŸ“ GPU (OpenGL/EGL) |

å¯¹äºå¤š GPU è®¾ç½®ï¼Œå§‹ç»ˆåŒæ—¶è®¾ç½®æ‰€æœ‰ä¸‰ä¸ªã€‚

## GPU é€‰æ‹©æ¨¡å¼

| æ¨¡å¼ | æ–¹æ³• | GPU | å¤æ‚åº¦ |
|---------|--------|------|------------|
| å• GPU | `gs.init(backend=gs.gpu)` | 1 | ä½ |
| æ‰¹å¤„ç†ç¯å¢ƒ | `scene.build(n_envs=N)` | 1 | ä½ |
| å¤šè¿›ç¨‹ | å¤šè¿›ç¨‹ + ç¯å¢ƒå˜é‡ | N | ä¸­ |
| åˆ†å¸ƒå¼ | torchrun + DDP | N | é«˜ |

## æœ€ä½³å®è·µ

1. **ä¼˜å…ˆæ‰¹å¤„ç†**ï¼šåœ¨æ‰©å±•åˆ°å¤š GPU ä¹‹å‰ï¼Œå…ˆåœ¨å•ä¸ª GPU ä¸Šä½¿ç”¨å¤§çš„ `n_envs`
2. **è®¾ç½®æ‰€æœ‰ç¯å¢ƒå˜é‡**ï¼šå§‹ç»ˆåŒæ—¶è®¾ç½® CUDAã€Taichi å’Œ EGL è®¾å¤‡
3. **åŒæ­¥ DDP**ï¼šåœ¨é”€æ¯è¿›ç¨‹ç»„ä¹‹å‰è°ƒç”¨ `dist.barrier()`
4. **æ— å¤´æ¸²æŸ“**ï¼šåœ¨æœåŠ¡å™¨ä¸Šè®¾ç½® `pyglet.options["headless"] = True`
5. **ç›‘æ§å†…å­˜**ï¼šåœ¨æ‰¹å¤„ç†ä»¿çœŸæœŸé—´ä½¿ç”¨ `nvidia-smi`

## è®¾å¤‡è®¿é—®

åˆå§‹åŒ–åï¼š

```python
gs.device    # PyTorch è®¾å¤‡ (ä¾‹å¦‚ "cuda:0", "mps:0")
gs.backend   # åç«¯ç±»å‹ (gs.cuda, gs.metal, gs.cpu)
```
